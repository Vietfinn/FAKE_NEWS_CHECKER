import logging
import random
import re
import time
import unicodedata
from collections import Counter
from typing import Dict, List, Optional, Tuple
from urllib.parse import quote_plus, urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from curl_cffi import requests as cffi_requests

# Import hàm chuẩn hóa từ file chúng ta vừa tạo
from text_utils import normalize_text

logger = logging.getLogger(__name__)


class Crawler:

    def __init__(self):
        logger.info("Crawler initialized")
        # Bạn có thể khởi tạo AsyncSession ở đây nếu dùng cho Giai đoạn 2

    def is_valid_article_url(self, url):
        invalid_patterns = [
            "/topic/",
            "/category/",
            "/tag/",
            "/search",
            "/tim-kiem",
            "/video/",
            "/podcast/",
            "/page/",
            "/chu-de/",
            "/folder/",
            "/gallery/",
            "/photo/",
        ]

        has_number = any(char.isdigit() for char in url)
        is_not_category = not any(
            pattern in url.lower() for pattern in invalid_patterns
        )

        url_path = url.split("/")[-1]
        has_sufficient_length = len(url_path) > 15

        return has_number and is_not_category and has_sufficient_length

    def extract_from_url(self, url):

        if not self.is_valid_article_url(url):
            logger.warning(f"URL may not be a valid article: {url}")

        result = self._try_requests_method(url)
        if result:
            return result

        logger.warning("Method 1 failed, trying archive.org proxy...")
        result = self._try_archive_method(url)
        if result:
            return result

        logger.warning("Method 2 failed, trying search snippet extraction...")
        result = self._try_search_snippet_method(url)
        if result:
            return result

        logger.error("All extraction methods failed")
        return None

    def _try_requests_method(self, url, max_retries=3):

        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        ]

        for attempt in range(max_retries):
            try:
                logger.info(
                    f"Attempt {attempt + 1}/{max_retries}: Extracting from {url} using curl_cffi"
                )

                headers = {
                    "User-Agent": random.choice(user_agents),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
                }

                domain = urlparse(url).netloc
                if "vnexpress.net" in domain:
                    headers["Referer"] = "https://www.google.com/search?q=vnexpress"

                response = cffi_requests.get(
                    url,
                    headers=headers,
                    timeout=30,
                    allow_redirects=True,
                    verify=True,
                    impersonate="chrome120",
                )

                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, "html.parser")

                    for tag in soup(
                        [
                            "script",
                            "style",
                            "iframe",
                            "noscript",
                            "nav",
                            "footer",
                            "header",
                        ]
                    ):
                        tag.decompose()

                    title = self._extract_title(soup)
                    description = self._extract_description(soup)
                    content = self._extract_content(soup, url)

                    if content and len(content) > 100:
                        logger.info(
                            f"Successfully extracted {len(content)} characters (via curl_cffi)"
                        )
                        return {
                            # THAY ĐỔI: Sử dụng hàm normalize_text được import
                            "title": normalize_text(title),
                            "description": normalize_text(description),
                            "content": normalize_text(content),
                            "url": url,
                            "domain": domain,
                        }

                logger.warning(
                    f"Attempt {attempt + 1} failed: Status {response.status_code}"
                )

            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")

            if attempt < max_retries - 1:
                time.sleep(random.uniform(2, 5))

        return None

    def _try_archive_method(self, url):

        try:

            archive_api = f"http://archive.org/wayback/available?url={url}"
            response = requests.get(archive_api, timeout=10)
            data = response.json()

            if "archived_snapshots" in data and "closest" in data["archived_snapshots"]:
                archive_url = data["archived_snapshots"]["closest"]["url"]
                logger.info(f"Found archive.org snapshot: {archive_url}")

                archive_response = requests.get(archive_url, timeout=30)
                if archive_response.status_code == 200:
                    soup = BeautifulSoup(archive_response.content, "html.parser")

                    for tag in soup(
                        [
                            "script",
                            "style",
                            "iframe",
                            "noscript",
                            "nav",
                            "footer",
                            "header",
                        ]
                    ):
                        tag.decompose()

                    title = self._extract_title(soup)
                    description = self._extract_description(soup)
                    content = self._extract_content(soup, url)

                    if content and len(content) > 100:
                        logger.info(
                            f"Archive.org extraction successful: {len(content)} chars"
                        )
                        return {
                            # THAY ĐỔI: Sử dụng hàm normalize_text được import
                            "title": normalize_text(title),
                            "description": normalize_text(description),
                            "content": normalize_text(content),
                            "url": url,
                            "domain": urlparse(url).netloc,
                        }
        except Exception as e:
            logger.warning(f"Archive.org method failed: {str(e)}")

        return None

    def _try_search_snippet_method(self, url):
        try:
            search_query = (
                f"site:{urlparse(url).netloc} {url.split('/')[-1].replace('-', ' ')}"
            )
            search_url = f"https://www.google.com/search?q={quote_plus(search_query)}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            response = requests.get(search_url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, "html.parser")

            search_divs = soup.find_all("div", class_="g")
            for div in search_divs:
                link = div.find("a", href=True)
                if link and url in link["href"]:
                    title_elem = div.find("h3")
                    title = title_elem.get_text(strip=True) if title_elem else ""

                    snippet_elem = div.find(
                        ["div", "span"], class_=re.compile("VwiC3b|s3v9rd")
                    )
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    if title and snippet and len(snippet) > 100:
                        logger.info(f"Search snippet extraction successful")
                        return {
                            # THAY ĐỔI: Sử dụng hàm normalize_text được import
                            "title": normalize_text(title),
                            "description": "",
                            "content": normalize_text(snippet),
                            "url": url,
                            "domain": urlparse(url).netloc,
                        }
        except Exception as e:
            logger.warning(f"Search snippet method failed: {str(e)}")

        return None

    def _extract_title(self, soup):
        title = ""
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text().strip()

        if not title:
            h1 = soup.find("h1")
            if h1:
                title = h1.get_text().strip()

        if not title:
            meta_title = soup.find("meta", property="og:title")
            if meta_title and meta_title.has_attr("content"):
                title = meta_title["content"].strip()

        return title

    def _extract_description(self, soup):
        description = ""
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if not meta_desc:
            meta_desc = soup.find("meta", attrs={"property": "og:description"})

        if meta_desc and meta_desc.has_attr("content"):
            description = meta_desc["content"].strip()

        return description

    def _extract_content(self, soup, url):
        content = ""
        domain = urlparse(url).netloc

        article = soup.find("article")
        if article:
            content = self._extract_paragraphs(article)
            if len(content) > 200:
                logger.info(f"Content found in <article> tag: {len(content)} chars")
                return content

        if not content or len(content) < 200:
            content_divs = soup.find_all(
                "div",
                class_=re.compile(
                    r"(content|article|body|detail|story|entry|post)", re.I
                ),
            )
            for div in content_divs:
                temp_content = self._extract_paragraphs(div)
                if len(temp_content) > len(content):
                    content = temp_content
                    if len(content) > 500:
                        logger.info(
                            f"Content found in content div: {len(content)} chars"
                        )
                        break

        if not content or len(content) < 200:
            content = self._extract_domain_specific(soup, domain)

        if not content or len(content) < 200:
            paragraphs = soup.find_all("p")
            texts = [
                p.get_text().strip()
                for p in paragraphs
                if len(p.get_text().strip()) > 30
            ]
            content = " ".join(texts[:50])
            logger.info(f"Fallback extraction: {len(content)} chars")

        return content

    def _extract_paragraphs(self, element):
        paragraphs = element.find_all("p")
        texts = [
            p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 30
        ]
        return " ".join(texts)

    def _extract_domain_specific(self, soup, domain):
        content = ""

        if "vnexpress.net" in domain:
            vnexpress = soup.find(["article", "div"], class_="fck_detail")
            if vnexpress:
                content = self._extract_paragraphs(vnexpress)
                logger.info(f"VnExpress content: {len(content)} chars")

        elif "tuoitre.vn" in domain:
            tuoitre = soup.find("div", id="main-detail-content")
            if tuoitre:
                content = self._extract_paragraphs(tuoitre)
                logger.info(f"Tuổi Trẻ content: {len(content)} chars")

        elif "thanhnien.vn" in domain:
            thanhnien = soup.find("div", class_=re.compile("content|detail|body", re.I))
            if thanhnien:
                content = self._extract_paragraphs(thanhnien)
                logger.info(f"Thanh Niên content: {len(content)} chars")

        elif "dantri.com.vn" in domain:
            dantri = soup.find("div", class_=re.compile("detail|content", re.I))
            if dantri:
                content = self._extract_paragraphs(dantri)
                logger.info(f"Dân Trí content: {len(content)} chars")
        elif "vietnamnet.vn" in domain:
            vietnamnet = soup.find(
                "div", class_=re.compile("main-content|article-content", re.I)
            )
            if vietnamnet:
                content = self._extract_paragraphs(vietnamnet)
                logger.info(f"VietnamNet content: {len(content)} chars")

        return content
